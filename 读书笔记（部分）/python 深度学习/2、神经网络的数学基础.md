# 神经网络的数学基础

[toc]

要理解深度学习,需要熟悉很多简单的数学概念：张量、张量运算、微分、梯度下降等。

```text
关于类和标签的说明
在机器学习中,分类问题中的某个类别叫作类(class) 。数据点叫作样本(sample) 。某个样本对应的类叫作标签(label) 。
```

**神经网络的核心组件是层(layer)** ,它是一种数据处理模块,你可以将它看成数据过滤器。
进去一些数据,出来的数据变得更加有用。具体来说,层从输入数据中提取表示——我们期望这种表示有助于解决手头的问题。

大多数深度学习都是将简单的层链接起来,从而实现渐进式的数据蒸馏(data distillation) 。**深度学习模型就像是数据处理的筛子,包含一系列越来越精细的数据过滤器(即层) 。 ** 

要想训练网络,我们还需要选择编译(compile)步骤的三个参数。

- **损失函数(loss function)** :网络如何衡量在训练数据上的性能,即网络如何朝着正确的方向前进。
- **优化器(optimizer)** :基于训练数据和损失函数来更新网络的机制。
- **在训练和测试过程中需要监控的指标(metric)** :本例只关心精度,即正确分类的图像所占的比例。

接下来来了解一些机器学习中的数据表示概念。

## 神经网络中的数据表示

#### 张量（tensor）

一般我们使用的数据存储在多维 Numpy 数组中，这种数据的集合也被称为张量。

当前所有的机器学习系统都使用张量作为基本数据结构。

**张量这一概念的核心在于,它是一个数据容器！**它包含的数据几乎总是数值数据,因此**它是数字的容器**。你可能对矩阵很熟悉,它是二维张量。

**张量是矩阵向任意维度的推广**[注意, 张量的维度(dimension)通常叫作轴(axis) ] 。

- **标量** 

  仅包含一个数字的张量就是标量

  ```python
  [1]

- **一维张量（向量）** 

  ```python
  [1, 2, 3, 4] # 1->9
  ```

  上面的例子中有 4 个元素，也被称为 **4D 向量（或者 1D 张量）**，D 指 dimension 维度。

- **二维张量（矩阵）**  
  $$
  \begin{bmatrix}
   1 & 2 & 3 \\
   4 & 5 & 6 \\
   7 & 8 & 9
  \end{bmatrix}
  $$

- **三纬张量** 

  将多个矩阵组合成一个新的数组可以得到一个 3D 张量，可以直观的**理解为数字的“立方体”**。

  ![image-20231010201236736](https://liaoyk-markdown.oss-cn-hangzhou.aliyuncs.com/markdownImg_2023/image-20231010201236736.png) 

在此基础上类推，将多个 3D 张量组合又可以得到 4D 张量。

#### 关键属性

- 轴的个数(阶) 。例如,3D 张量有 3 个轴,矩阵有 2 个轴。**这在 Numpy 等 Python 库中也叫张量的 `ndim`。** 
- 形状。这是一个整数元组,表示张量沿每个轴的维度大小(元素个数) 。例如,前面矩阵示例的形状为 (3, 3)即总共 2个轴 2 项，每个轴上都是 3 个元素。3D 张量示例的形状为 (2, 3, 4) 即总共 3 个轴，轴的大小分别为 2，3，4。**向量的形状只包含一个元素,比如 (5,),而标量的形状为空,即 ()。**  
- 数据类型(在 Python 库中通常叫作 `dtype`) 。这是张量中所包含数据的类型,例如,**张量的类型可以是 float32、uint8、float64** 等。在极少数情况下,你可能会遇到字符(char)张量。注意,Numpy(以及大多数其他库)中不存在字符串张量,因为张量存储在预先分配的连续内存段中,而字符串的长度是可变的,无法用这种方式存储

## 在 numpy 中操作张量

我们可以直接使用`[]`访问张量中的特定元素，这被称为**张量切片**！

```python
import numpy as np

tensor_2d = np.array(
    [
        [1, 2],
        [3, 4]
    ]
)

print(tensor_2d[0]) # [1, 2] 访问了第 0 个轴
print(tensor_2d[1]) # [3, 4] 访问了第 1 个轴
```

还可以进行更精细的访问，访问轴上指定的元素

```python
print(tensor_2d[0, :1]) # [1] 访问了第 0 个轴上的第 1 个元素
```

#### 批量轴（批量维度）

在处理数据集时通常不会一下处理所有数据，而是将他们分成一小块一小块。

比入一次处理 128 个数据

```python
batch = train_image[:128]
```

之后每次都递增的处理这么多数据。对于这种处理方式，**我们把第一个轴也就是第一组数据称为批量轴或者批量维度。** 

#### 用张量描述现实世界

这里用🌰来说明：

- 人口统计数据集：其中包括每个人的**年龄**、**邮编**和收入。**每个人**可以表示为**包含 3 个值的向量,**而**整个数据集包含 100 000 个人**,因此可以存储在形状为` (100000, 3) `的 2D 张量中。
- 文档数据集：我们将每个文档表示为每个单词在其中出现的次数(字典中包含 20 000 个常见单词) 。每个文档可以被编码为包含 20 000 个值的向量(每个值对应于字典中每个单词的出现次数) ,整个数据集包含 500 个文档,因此可以存储在形状为(500, 20000) 的张量中。

##### 时间序列数据

如果数据中需要包含时间，那么我们就需要 3D 张量。

![image-20231011185330528](https://liaoyk-markdown.oss-cn-hangzhou.aliyuncs.com/markdownImg_2023/image-20231011185330528.png) 

根据惯例,时间轴始终是第 2 个轴(索引为 1 的轴) 。我们来看几个例子。

- 股票价格数据集。每一分钟,我们将股票的**当前价格(1)**、**前一分钟的最高价格(2)**和**前一分钟的最低价格(3)**保存下来。因此每分钟被编码为一个 3D 向量,整个交易日被编码为一个形状为` (390, 3) `的 2D 张量(一个交易日有 390 分钟) ,而 250 天的数据则可以保存在一个形状为` (250, 390, 3)` 的 3D 张量中。

##### 图像数据

图像通常具有三个维度:高度、宽度和颜色深度。虽然灰度图像(比如 MNIST 数字图像) 只有一个颜色通道,因此可以保存在 2D 张量中,但按照惯例,图像张量始终都是 3D 张量。

因此,如果图像大小为 256×256,那么 128 张灰度图像组成的批量可以保存在一个形状为` (128, 256, 256, 1) `（只有灰度）的张量中。

128 张彩色图像组成的批量则可以保存在一个形状为`(128,256,256,3)`（红黄蓝 3 通道）的张量中。

##### 视频数据

视频数据则要用到 5D 张量，因为相比图像视频又多了帧的概念。

一系列帧可以保存在一个形状为`(frames, height, wiath, color_depth)`的4D 张量中。

举个🌰：一个以每秒4帧采样的 60秒You Tube 视频片段,视频尺寸为 144 ×256,这个视频共有 240 帧。4个这样的视频片段组成的批量将保存在形状为` (4, 240, 144, 256,3)`的张量中。这样一段视频总共有`4*240*144*256*3 = 106168320` 个值，如果张量的数据类型(atype)是float32,每个值都是32 位,那么这个张量共有 405MB（106168320*32/8/1024/1024）。

## 张量运算—神经网络的齿轮

张量的运算基础是数学中的线性代数运算，参照矩阵的数学运算。

在编程中我们不需要把精力放在研究数学知识上，一般我们接住 Numpy 库来帮助我们运算。我们需要做的是了解 Numpy 中的常用运算方法。

#### 手动逐元素运算

relu 运算（max）和加法运算都是逐元素的运算，在直接借助 Numpy 进行运算之前，我们先了解一下他的具体实现。其实很简单，就是通过 for 循环遍历所有的元素，逐个处理。

```python
def naive_add(x, y):
  assert len(x.shape) == 2  # x 和 y 是 2D Numpy 张量
  assert x.shape == y.shape
  
  x = x.copy()  # 避免覆盖输入张量
  for i in range(x.shape[0]):
    for j in range(x.shape[1]):  # 2D 张量，需要逐维遍历，所以有 2 层
      x[i, j] += y[i, j]
  return x
```

#### Numpy 运算

在实践中处理 Numpy 数组时,这些运算都是优化好的 Numpy 内置函数,这些函数将大量运算交给安装好的基础线性代数子程序(BLAS,basic linear algebra subprograms)实现。

所以我们可以直接使用简单的操作符来操作。

```python
import numpy as np

data1 = np.array([[1, 2, 3], [4, 5, 6]])
data2 = np.array([[1, 2, 3], [4, 5, 6]])

addedData = data1 + data2 # 加法运算
np.maximum(data1, 2,) # max 运算，取元素和 2 中比较大的那一个
```

#### 广播

上面的例子中，用来运算的张量形状是相同的，都是`(2, 3)`。如果形状不同呢？那么较小的张量就会**被广播以匹配较大张量的形状**。

举🌰来看：

```python
data1 = np.array([[2, 3, 4]])
data2 = np.array([[2, 3, 4], [4, 5, 6]])
```

data1 张量的形状为`(1, 3)` 

data2 的张量形状为`(2, 3)` 

他们相加时，data1 会被广播（添加轴），**使其 ndim 与较大的张量相同。** 

具体表现为 data1 变为如下形状：

```python
data1 = np.array([[2, 3, 4], [2, 3, 4]])
```

整个过程不会真的在内存中执行如此的转换，这种运算只存在于算法中。

#### 张量点积（重要）

点积运算,也叫张量积(tensor product,**不要与逐元素的乘积弄混**) ,是最常见也最有用的张量运算。

在数学中逐元素的乘积运算被称为哈达玛积(Hadamard product)，这里肯定不是哈达马积运算，而是**和矩阵相乘一样**。

点积的运算方式用代码表示如下：

```python
def naive_vector_dot(x, y):
  assert len(x.shape) == 1  # x 和 y 是 Numpy 向量
  assert len(y.shape) == 1
  assert x.shape[0] == y.shape[0]

  z = 0. # 向量之基是一个标量
  for i in range(x.shape[0]):
    z += x[i] * y[i]
  return z
```

和数学运算一样，要想进行点积运算，前一个张量的行数要和后一个张量的列数相等。

**即当且仅当` x.shape[1] == y.shape[0] `时,你才可以对它们做点积`(dot(x, y))`** 。得到的结果是一个形状为 (x.shape[0], y.shape[1]) 的矩阵。

我们可以用一张图片来理解点积运算，下面是两个向量的点积:

![image-20231011201815009](https://liaoyk-markdown.oss-cn-hangzhou.aliyuncs.com/markdownImg_2023/image-20231011201815009.png)  
